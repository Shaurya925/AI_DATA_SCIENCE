# -*- coding: utf-8 -*-
"""AI_Data_Science.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tOWrSDq9b6vuBGeYLy1Fg4S5RJ1yvGKZ
"""

#variables and input
name =str(input("what is your name"))
temp = 25
carbon_footprint = 500.75
is_sustainable = carbon_footprint < 400
print(f"Hello {name} Today temprature is {temp}°C")
print(f"Carbon Footprint: {carbon_footprint}")
print(f"Is the city sustainable: {is_sustainable}")

#list
weekly_temp = [25,27,28,26,24,30,29]
print(weekly_temp)

city_data = {
    "city_name": "New York",
    "temperature": 25,
    "carbon_footprint": 500.75,
    "is_sustainable": False
}
print(f"City Name: {city_data}")
#using dictionaries we can use pandas to visualise data in a tabular form

#assigning multiple variables
city_name,temperature,carbon_footprint,is_sustainable = "City B",30,500.75,False
print(f"City Name: {city_name} Temp: {temperature}°C")
print(f"Carbon Footprint: {carbon_footprint} KG CO2")
print(f"Is the city sustainable: {is_sustainable}");

#control Statements
count = 5
if count > 0:
    print("Count is greater than zero")
else:
    print("Count is not greater than zero")

#function creation and implementation
def add(x,y):
    return x+y

result = add(int(input("number1")),int(input("number2")))

print(result)

x =lambda a,b:a+b; #lamda is an anonyomous function which can take number of parameters but only return one expression hence it does not need any return keyword to return the value
a = int(input("number1"))
b = int(input("number2"))
print(x(a,b))

#list comprehension - used to filtering the data from the given list of objects based on a particular condition
climate_data = [
    {
    "city_name": "City A",
    "temperature": 25,
    "carbon_footprint": 500.75,
    },
    {
    "city_name": "City B",
    "temperature": 30,
    "carbon_footprint": 700.50,
    },
    {
    "city_name": "City C",
    "temperature": 28,
    "carbon_footprint": 600.25,
    },
    {
    "city_name": "City D",
    "temperature": 22,
    "carbon_footprint": 400.00,
    }
]
temp_threshold = 26

high_temp_cities= [city for city in climate_data if city["temperature"] > temp_threshold]

# Iterate through the list and print the city names
for city in high_temp_cities:
    print(f"{city['city_name']} -{city['temperature']}°C")

#caluculating average carbon footprint
climate_data = [
    {
    "city_name": "City A",
    "temperature": 25,
    "carbon_footprint": 500.75,
    },
    {
    "city_name": "City B",
    "temperature": 30,
    "carbon_footprint": 700.50,
    },
    {
    "city_name": "City C",
    "temperature": 28,
    "carbon_footprint": 600.25,
    },
    {
    "city_name": "City D",
    "temperature": 22,
    "carbon_footprint": 400.00,
    }
]

total_carbon = 0

for city in climate_data:
    total_carbon += city["carbon_footprint"]

average_carbon = total_carbon / len(climate_data)

print(f"Total Carbon Footprint: {total_carbon}")
print(f"Average Carbon Footprint: {average_carbon}")

#use of lambda for filtering the the data for sustainable cities
climate_data = [
    {
    "city_name": "City A",
    "temperature": 25,
    "carbon_footprint": 500.75,
    },
    {
    "city_name": "City B",
    "temperature": 30,
    "carbon_footprint": 700.50,
    },
    {
    "city_name": "City C",
    "temperature": 28,
    "carbon_footprint": 600.25,
    },
    {
    "city_name": "City D",
    "temperature": 22,
    "carbon_footprint": 400.00,
    }
]
sustainiblity_threshold = 600

high_carbon_cities = list(filter(lambda city: city["carbon_footprint"] < sustainiblity_threshold, climate_data))

for city in high_carbon_cities:
    print(f"{city['city_name']} - {city['carbon_footprint']} CO2 KG")

#calculating hhighest and lowest footprint using lambda function
climate_data = [
    {
    "city_name": "City A",
    "temperature": 25,
    "carbon_footprint": 500.75,
    },
    {
    "city_name": "City B",
    "temperature": 30,
    "carbon_footprint": 700.50,
    },
    {
    "city_name": "City C",
    "temperature": 28,
    "carbon_footprint": 600.25,
    },
    {
    "city_name": "City D",
    "temperature": 22,
    "carbon_footprint": 400.00,
    }
]

highest_footprint = max(climate_data,key = lambda city:city["carbon_footprint"])

lowest_footprint = min(climate_data,key = lambda city:city["carbon_footprint"])



print(f"City with the highest carbon footprint: {highest_footprint['city_name']} - {highest_footprint['carbon_footprint']}")

print(f"City with the lowest carbon footprint:  {lowest_footprint['city_name']} - {lowest_footprint['carbon_footprint']}")

#introduction to pandas(Data in tabular form)
#dictionaries are converted into tabular form by using pandas Its is important as ml models as datasets are required to train the model
#open soure data manipulation built on top of NumPy
#merging and joining datasets,data cleaning,tarnsformationand aggregation
#provides 2 main data structures: Series(1D->1 excel column) and DataFrames(2D->1 excel column and row)
#APPLIICATIONS:Data Handling(importing csv or excel file and hanlde missing data,Data Wrangling,EDA)
#---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

def calucateCfootprint(engery_consumption,emission_factor):
    return engery_consumption * emission_factor

print(f"{calucateCfootprint(1000,0.475)} KG CO2")

!pip install pandas

import pandas as pd
renewable_resources  = ["Solar","Wind","Hydropower","Geothermal","Biomass"]
data = {
    "Project":["Solar Farm A","Wind Farm B","Hydropower Plant C","Geothermal Plant D","Solar Farm B"],
    "Technology":["Solar","Wind","Hydropower","Geothermal","Solar"],
    "Capacity(MW)":[150,300,200,50,100],
    "Cost(Million $)":[200,400,350,100,250],
    "Location":["California","Texas","Florida","New York","California"],
    "Completion Year":[2023,2022,2023,2022,2023],
    # "Renewable Resources":renewable_resources
}

series = pd.Series(renewable_resources)
print("Renewable_Resources:")
print(series)

print("\n Green Technology Dataframe:")
DataFrame = pd.DataFrame(data)
print(DataFrame.Project)

#filtering the high capacity projects
import pandas as pd
renewable_resources  = ["Solar","Wind","Hydropower","Geothermal","Biomass"]
data = {
    "Project":["Solar Farm A","Wind Farm B","Hydropower Plant C","Geothermal Plant D","Solar Farm B"],
    "Technology":["Solar","Wind","Hydropower","Geothermal","Solar"],
    "Capacity(MW)":[150,300,200,50,100],
    "Cost(Million $)":[200,400,350,100,250],
    "Location":["California","Texas","Florida","New York","California"],
    "Completion Year":[2023,2022,2023,2022,2023],
    # "Renewable Resources":renewable_resources
}


DataFrame = pd.DataFrame(data)
high_capacity = DataFrame[DataFrame["Capacity(MW)"] > 100]
print(high_capacity)

#Adding columns
import pandas as pd
renewable_resources  = ["Solar","Wind","Hydropower","Geothermal","Biomass"]
data = {
    "Project":["Solar Farm A","Wind Farm B","Hydropower Plant C","Geothermal Plant D","Solar Farm B"],
    "Technology":["Solar","Wind","Hydropower","Geothermal","Solar"],
    "Capacity(MW)":[150,300,200,50,100],
    "Cost(Million $)":[200,400,350,100,250],
    "Location":["California","Texas","Florida","New York","California"],
    "Completion Year":[2023,2022,2023,2022,2023],
    # "Renewable Resources":renewable_resources
}


DataFrame = pd.DataFrame(data)
DataFrame["Cost per MW"] = DataFrame["Cost(Million $)"] / DataFrame["Capacity(MW)"]
print("\n DATAFRAME UPDATED ONE")
print(DataFrame)

#Finding sums
import pandas as pd
renewable_resources  = ["Solar","Wind","Hydropower","Geothermal","Biomass"]
data = {
    "Project":["Solar Farm A","Wind Farm B","Hydropower Plant C","Geothermal Plant D","Solar Farm B"],
    "Technology":["Solar","Wind","Hydropower","Geothermal","Solar"],
    "Capacity(MW)":[150,300,200,50,100],
    "Cost(Million $)":[200,400,350,100,250],
    "Location":["California","Texas","Florida","New York","California"],
    "Completion Year":[2023,2022,2023,2022,2023],
    # "Renewable Resources":renewable_resources
}


DataFrame = pd.DataFrame(data)
total_cost = DataFrame["Cost(Million $)"].sum()
total_capacity = DataFrame["Capacity(MW)"].sum()
print(f"Total Cost: {total_cost} Million $")
print(f"Total Capacity: {total_capacity} MW")

#Finding sums
import pandas as pd
renewable_resources  = ["Solar","Wind","Hydropower","Geothermal","Biomass"]
data = {
    "Project":["Solar Farm A","Wind Farm B","Hydropower Plant C","Geothermal Plant D","Solar Farm B"],
    "Technology":["Solar","Wind","Hydropower","Geothermal","Solar"],
    "Capacity(MW)":[150,300,200,50,100],
    "Cost(Million $)":[200,400,350,100,250],
    "Location":["California","Texas","Florida","New York","California"],
    "Completion Year":[2023,2022,2023,2022,2023],
    # "Renewable Resources":renewable_resources
}


DataFrame = pd.DataFrame(data)
max_capacity = DataFrame["Capacity(MW)"].max()
min_capacity = DataFrame["Capacity(MW)"].min()
print(f"Maximum Capacity: {max_capacity} MW")
print(f"Minimum Capacity: {min_capacity} MW")

#Grouped data
import pandas as pd
renewable_resources  = ["Solar","Wind","Hydropower","Geothermal","Biomass"]
data = {
    "Project":["Solar Farm A","Wind Farm B","Hydropower Plant C","Geothermal Plant D","Solar Farm B"],
    "Technology":["Solar","Wind","Hydropower","Geothermal","Solar"],
    "Capacity(MW)":[150,300,200,50,100],
    "Cost(Million $)":[200,400,350,100,250],
    "Location":["California","Texas","Florida","New York","California"],
    "Completion Year":[2023,2022,2023,2022,2023],
    # "Renewable Resources":renewable_resources
}


DataFrame = pd.DataFrame(data)
grouped_data = DataFrame.groupby("Technology")["Capacity(MW)"].sum()
print(grouped_data)

# NUMPY-> Numerical python:foundational package for numerical computing
# Efficient handling of numerical data
# Essential for scientific calculations and data analysis optimised for high speed datadata analysis
# Major role in image processing Mostly Numpy and Pandas go hand in hand with each other also works seamlessly with matplotlib and Scipy
# ndim -> rows  shape -> columns rand -> random values array
# array,.randint(gives in random order),.unique,.dot,.power,.sqrt,.ones,.zeros,.max,.arange(creates array with evenly spaced)

!pip install Numpy

import numpy as np
energy_consumption = np.array([1200,3400,2900,1800,2500])
sum = np.sum(energy_consumption)
mean = np.mean(energy_consumption)
reshaped = energy_consumption.reshape(5,1)
#std the diff btw mean value and observations
stdvation = np.std(energy_consumption)
print((energy_consumption))
print(sum)
print(mean)
print(stdvation)
print(reshaped)

"""# Prerequisites for Machine Learning:
# Data Preprocessing(EDA) -> Transforming raw data into clean,organised and usable format suitable analysis.It includes cleaning structing and ensure quality and consistency




# *4 key Types:*



# 1 -> Discovery:Understanding Data
# 2 -> Transformation:Making Data suitable for analysis
# 3 -> Validation:Ensuring accuracy of data
#  4 -> Publishing:Preparing the processed data for usage

Prerequisites for Machine Learning:
# Data Preprocessing(EDA):
# to convert the type of data of different columns into its suitable:Example:df ["Age"].astype(int) to check we can use df.info
# checking for missing values in the dataset(kaggle,github,paperswithcode) using df.isnull().sum()
 # If the missing values ae 5% we can negelect it:to Drop It we can use df.dropna() it is known as imputing(filling the missing values) missing values using df.fillna(df["age"]).mean(),df["dept"].unique
 # Normalisation/Scaling(should only be done for numeric columns): Standard Scalar(using z-score so that the difference between value sint ls large) and MinMax scaling
 # Encoding of the clean data
 # Checking For Outliers:boxplot for checking outliers
"""

#before the data fed to ml model it should be in numerical data form, it can only be build with numerical data
# In preprocessing all the categorical columns should be converted into numerical columns
# Encoding techniques one hat coding label encoding odinal enconding and target encoding
# pd.getdummies() used for one hat coding
# df.replace([0,/,?,;],np.nan) is necessray as it dont contribute to  model building
# To check unique values we can use df.unique()
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler

data = {
    "Energy Source":["Solar","Wind","Hydropower","Geothermal","Biomass","Nuclear"],
    "Energy Consumption":[1200,np.nan,2900,np.nan,2500,3200],
    "Cost":[200,400,np.nan,150,250,np.nan]
}

DataFrame1 = pd.DataFrame(data)
print(DataFrame1)
print("--------------------------")
#Handling Missing values
null_count = DataFrame1.isnull().sum()
print(null_count)
print("--------------------------")

# By dropping missing values
# cleaned = DataFrame.dropna()
# print(cleaned)  (Its of no use since the missing values are more than 5%)

# forward Filling
forwardfill = DataFrame1.ffill()
# print(forwardfill)

# Imputing missing values by mean values
DataFrame1["Energy Consumption"] = DataFrame1["Energy Consumption"].fillna(DataFrame1["Energy Consumption"].mean())
DataFrame1["Cost"] = DataFrame1["Cost"].fillna(DataFrame1["Cost"].mean())
print(DataFrame1)
print("--------------------------")

#MinMaxScaling
Mscaler = MinMaxScaler()
DataFrame1[["Energy Consumption","Cost"]] = Mscaler.fit_transform(DataFrame1[["Energy Consumption","Cost"]])
print(DataFrame1)
print("--------------------------")

#z-score scaling
SScaler = StandardScaler()
DataFrame1[["Energy Consumption","Cost"]] = SScaler.fit_transform(DataFrame1[["Energy Consumption","Cost"]])
print(DataFrame1)
print("--------------------------")

#One hat encoding
encoded = pd.get_dummies(DataFrame1["Energy Source"])
print(encoded)
print("--------------------------")

#Lastly combined data
combined = pd.concat([DataFrame1,encoded],axis = 1)
print(combined)

"""##DATA VISUALISATION
#MATPLOTLIB/Seaborn:
It is used for creating static,animated and interactive visualisations:
#plots:
line,bar,scatter,histogram
plot(),scatter(),hist(),hist2d()

# from matplotlib import pyplot as plt
#.plot(x,y) used for plotting
#.title:graph title
#.ylabel() y axis name
#.xlabel() x axis name
#.show() to show graph
"""

from matplotlib import pyplot as plt
months = ["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"]
energy_usage = [1200,1300,1100,1400,1600,1700,1200,1000,1800,2000,1350,2100]
plt.plot(months,energy_usage, marker = 'o',color = 'b', linestyle = 'dashed')
plt.title("Monthly Energy Usage")
plt.ylabel("Energy Usage(kWh)")
plt.xlabel("Months")
plt.show()

from matplotlib import pyplot as plt
months = ["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"]
energy_usage = [1200,1300,1100,1400,1600,1700,1200,1000,1800,2000,1350,2100]
plt.bar(months,energy_usage,color = "r")
plt.title("Monthly Energy Usage")
plt.ylabel("Energy Usage(kWh)")
plt.xlabel("Months")
plt.show()

from matplotlib import pyplot as plt
months = ["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"]
energy_usage = [1200,1300,1100,1400,1600,1700,1200,1000,1800,2000,1350,2100]
plt.scatter(months,energy_usage,color = "g")
plt.title("Monthly Energy Usage")
plt.ylabel("Energy Usage(kWh)")
plt.xlabel("Months")
plt.show()

from matplotlib import pyplot as plt
months = ["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"]
energy_usage = [1200,1300,1100,1400,1600,1700,1200,1000,1800,2000,1350,2100]
plt.hist(energy_usage,bins = 5,color = "y")
plt.title("Monthly Energy Usage")
plt.ylabel("Energy Usage(kWh)")
plt.xlabel("Months")
plt.show()

from matplotlib import pyplot as plt
months = ["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"]
energy_usage = [1200,1300,1100,1400,1600,1700,1200,1000,1800,2000,1350,2100]
plt.pie(energy_usage,labels = months,autopct = "%1.1f%%")
plt.title("Monthly Energy Usage")
plt.ylabel("Energy Usage(kWh)")
plt.xlabel("Months")
plt.show()

"""# Machine Learning And Deep Learning

-> Overview of AI and Ml

---
#Functionality Based

*   Reactive Machine AI
*   Limited Memory AI
*   Theory of mind AI(Sense human emotions)
*   Self Aware AI(Final stage of AI)

---
#Machine Learning Types

*   Supervised Learning
*   Unsupervised Learning
*   Reinforcement Learning
*   Semi-supervised Learning

---
#Workflow
1.   Define The problem
2.   Collect and preprocess the data
3.   Data Spiltting
4.   Algorithim Selection
5.   Develop Machine Learning Model(Model Training)
6.   Test(Evaluation) and Refine the model(Optimisation)
7.   Final Model Evaluation or Test Data
8.   Deploy Machine Learning Prediction Service
9.   Manage Prediction Service
---










"""

# Data Spiltting(Done after preprocessing of data) training:80% testing:20%
# X->Independent Variable/predictive
# y -> Dependent variable /target
# Feature Importance
# Model selection(Logistics Regression)
# Training the Model (model.fit(X_train_scaled,y_train))
# Evaluate(model.Evaluate)
# Hyper Parameter Tuning
# Model Saving and Deployment(import joblib.dump(),import pickle)


from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2,random_state = 42)

"""# Types of ML


1.   Supervised Learning :)
    1. Regression     (If the target is numerical)  
    2. Classification (If the target is categorical)

2.   Unsupervised Learning :)

Mathematics behind regression:
Y = b+b1x1+b2x2 +....+ bnxn + e
where e = error b = intercept
b1..n = cofficient
x1..n = Independent Variables

---
Errors:
MAE,MSE,RMSE,R2,Adjusted R-squared(only for regresison)

---
In Unsupervised ML we can make only clusters(grouping) based on similarities

# **LINEAR REGRESSION**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score

df = pd.read_csv("/content/appliance_energy (1).csv")
print(df.head())
print("---------------------")
print(df.isnull().sum())

#Independent Variable
X = df['Temperature (°C)']
#Dependent Variable
y = df['Energy Consumption (kWh)']
# Data Spiltting
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2,random_state = 42)

# Create Linear Regression Model
model = LinearRegression()
#Train the model
model.fit(X_train.values.reshape(-1,1),y_train)
y_pred = model.predict(X_test.values.reshape(-1,1))
print(y_pred)

#calucate error
#Mean squared error
mse = mean_squared_error(y_test,y_pred)
#R-Squared Score
r2 = r2_score(y_test,y_pred)
print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")

plt.scatter(X_test,y_test,color = "b")
plt.plot(X_test,y_pred,color = "c")
plt.xlabel("Temperature (°C)")
plt.ylabel("Energy Consumption (kWh)")
plt.legend()
plt.title("Energy Consumption vs Temperature")
plt.show()

"""# **Evaluation Matrix**
acc = tp+tn/(tp+fp+fn+tn)
---
tp = true +ve
---
tn = true -ve
---
fn = false -ve
---
fp = false +ve
---
These values are given by confusion matrix which we can use using seabrone library

"""

# accuracy = accuracy_score(y_test,y_pred)

"""---
Deep Learing is based on nueral network,Each nueral network has number of nuerons(weights)

---
Each Nueral network has 3 layers

---

 1. input layer
 2. hidden layer
 3. output layer

---

Deep Learning model learns on its own.
It is highly accurate and doesnt require human intervention

---
Nuerons And Layers
---
1. Nuerons(Basic Building Blocks of Nueral Networks)
2. Layers(Groups of Nuerons)
3. Activation functions(introduce non lineraity to model, enables it to learn complex relationships)

---
Popular libraries => tensorflow, kerays, Pytorch
---
1epoh -> complete cycle of backfeeding and forwardfeeding this single cycle is known as 1 epoh

---
For hidden layers we use ReLU(Rectified Linear Unit) -> 0-maximum

---
Sigmoid - 0 nd 1

---
tanH ( -1 - 1)

---
SoftMax - MultiClass

---

# **Image Generation Using GenAI:**
---

OpenCV is an open soure library used for Image Processing

---
Image Augmentation -> Preprocessing the images

---
Segmentation - Dividing the image into meaningful regions

---
Teachable Machine uses a straightforward process where user can train a model with its own labeled dataset
"""

